{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'frame': None,\n",
       " 'feature_names': ['pixel_0_0',\n",
       "  'pixel_0_1',\n",
       "  'pixel_0_2',\n",
       "  'pixel_0_3',\n",
       "  'pixel_0_4',\n",
       "  'pixel_0_5',\n",
       "  'pixel_0_6',\n",
       "  'pixel_0_7',\n",
       "  'pixel_1_0',\n",
       "  'pixel_1_1',\n",
       "  'pixel_1_2',\n",
       "  'pixel_1_3',\n",
       "  'pixel_1_4',\n",
       "  'pixel_1_5',\n",
       "  'pixel_1_6',\n",
       "  'pixel_1_7',\n",
       "  'pixel_2_0',\n",
       "  'pixel_2_1',\n",
       "  'pixel_2_2',\n",
       "  'pixel_2_3',\n",
       "  'pixel_2_4',\n",
       "  'pixel_2_5',\n",
       "  'pixel_2_6',\n",
       "  'pixel_2_7',\n",
       "  'pixel_3_0',\n",
       "  'pixel_3_1',\n",
       "  'pixel_3_2',\n",
       "  'pixel_3_3',\n",
       "  'pixel_3_4',\n",
       "  'pixel_3_5',\n",
       "  'pixel_3_6',\n",
       "  'pixel_3_7',\n",
       "  'pixel_4_0',\n",
       "  'pixel_4_1',\n",
       "  'pixel_4_2',\n",
       "  'pixel_4_3',\n",
       "  'pixel_4_4',\n",
       "  'pixel_4_5',\n",
       "  'pixel_4_6',\n",
       "  'pixel_4_7',\n",
       "  'pixel_5_0',\n",
       "  'pixel_5_1',\n",
       "  'pixel_5_2',\n",
       "  'pixel_5_3',\n",
       "  'pixel_5_4',\n",
       "  'pixel_5_5',\n",
       "  'pixel_5_6',\n",
       "  'pixel_5_7',\n",
       "  'pixel_6_0',\n",
       "  'pixel_6_1',\n",
       "  'pixel_6_2',\n",
       "  'pixel_6_3',\n",
       "  'pixel_6_4',\n",
       "  'pixel_6_5',\n",
       "  'pixel_6_6',\n",
       "  'pixel_6_7',\n",
       "  'pixel_7_0',\n",
       "  'pixel_7_1',\n",
       "  'pixel_7_2',\n",
       "  'pixel_7_3',\n",
       "  'pixel_7_4',\n",
       "  'pixel_7_5',\n",
       "  'pixel_7_6',\n",
       "  'pixel_7_7'],\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = digits[\"data\"], digits[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know how many digits we have we can run this simple code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digit 0 appear 178 times\n",
      "digit 1 appear 182 times\n",
      "digit 2 appear 177 times\n",
      "digit 3 appear 183 times\n",
      "digit 4 appear 181 times\n",
      "digit 5 appear 182 times\n",
      "digit 6 appear 181 times\n",
      "digit 7 appear 179 times\n",
      "digit 8 appear 174 times\n",
      "digit 9 appear 180 times\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for i in range(10):\n",
    "    print (\"digit\", i, \"appear\", np.count_nonzero(y == i), \"times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: datasets loaded by scikit-learn have a dictionary structure. \n",
    "\n",
    "- a DESCR structure describing the dataset\n",
    "- a ```data``` key containing an array with one row per instance and one column per feature\n",
    "- a ```target``` with an array with the labels\n",
    "\n",
    "we have 1797 digits available, from 0 to 9. Let's define a helper function that will allow us to visualize the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADtUlEQVR4nO3dsXEqSRhG0WbrBUIGIgRCUSgKgRCkDAiBEAiBEFAEs9Z6Ksl5+3MLnWPOGN9g3OoqnN5t27aAnn8e/QHA18QJUeKEKHFClDgh6s8P78f+yr1er1NT63Q6jW2ttdb9fn/KrUmvr6/PvLf76qGTE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVE/Xccw5nw+j21dLpexrbXWOhwOY1vH43Fsa9J+v3/0J4xzckKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCEqcx3DpOkrC571OobJ3/UbOTkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6I+pV3pXx8fIzuXS6Xsa23t7exrfP5PLY1fb9NgZMTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUbtt2757/+3Lv+l2u01Njdvv92Nb7+/vY1vX63Vs63Q6jW09wO6rh05OiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRP159Af8Z/LKgmf2rFc//EZOTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0RlrmO43+9jW6fTaWxrrbWOx+PY1uQVCa7Q+H85OSFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTojabdv23ftvX/5Nt9ttamodDoexrbXW+vz8HNt6eXkZ2zqfz2NbT34vy+6rh05OiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRP10HQPwIE5OiBInRIkTosQJUeKEKHFC1L95Okn+Ng/dDQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_digit(some_digit):\n",
    "    \n",
    "    some_digit_image = some_digit.reshape(8,8)\n",
    "\n",
    "    plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_digit(X[1003])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the relative label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1003]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test set preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first reduce our dataset only to 0 and 1 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_01 = X[np.any([y == 0,y == 1], axis = 0)]\n",
    "y_01 = y[np.any([y == 0,y == 1], axis = 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a couple of digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAD6klEQVR4nO3dsVFjSRhG0WYLH4VABogMyABSIANCIQRCEBEAEYgQRAYQgdYaj2KcqV93Z88x9YzvybjVVTLUZ8fjcQE9/5z6BYDviROixAlR4oQocULU+W+e/5U/5b6+vo7uPTw8jG1tNpuxrd1uN7Y1+b1O4Oy7D52cECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiDr7zeW5Y9cxHA6Hqam13W7HttaavUrg4+NjbOvq6mps6/39fWzrBFzHAP8l4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSo81O/wC+vr69jW19fX2Nba6212+3Gtiavtbi/vx/ben5+Httaa63b29vRve84OSFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTojK3JUyecfHxcXF2NZaa22327Gtm5ubsa2np6exrf1+P7a1lrtSgB+IE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6Iy1zFMmrweYa21NpvN6B5/BycnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTojLXMVxfX49t7Xa7sa1pn5+ff+XW/5GTE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVFnx+Pxp+c/PvyTJv/a//LycmxrrbXu7u7Gtg6Hw9jW29vb2NZ+vx/bWmut7XY7OXf23YdOTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LOT/0Cv2w2m7Gtx8fHsa211rq/vx/burq6Gtt6eXkZ2xq+uyTByQlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSos+PxeOp3AL7h5IQocUKUOCFKnBAlTogSJ0T9C1TcUmiAsvj4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAD20lEQVR4nO3dsXFiSRhG0ceWfIUgQiADKQMIgRSUwWSgVCACEQIZiBBQBKy1HjXj7P7cVZ1j8oyvZdzqKjm9ut1uC9Dz16MPANwnTogSJ0SJE6LECVFPf/j+I/+VezweR/c+Pj7Gtg6Hw4/c2u/3Y1sPsLr3o5sTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUas/PJ77I59j2O12o3vX63Vs63Q6jW2tVndfEfhPfH19jW0ty7Ks1+vJOc8xwP+JOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCHq6dEHeITj8Ti6dzgcRvemvL6+jm1dLpexrWUZf47hLjcnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUZm3Us7n89jW8/Pz2NayLMt2ux3dm/L29vboI/xobk6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEZZ5jOJ1OY1ubzWZs6yebfI5h8rmOZWk8NeHmhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtTqdrv97vtvP/6bLpfL1NT4cwyTf9uk3W43tvX+/j62tSzLst1uJ+dW9350c0KUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCHq6dEH+Md6vR7b+v7+HttalmXZ7/djW9frdWzrfD6Pbb28vIxtVbg5IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiMq8lTLp8/NzdO/Xr1+je1NOp9PY1mazGduqcHNClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghanW73R59BuAONydEiROixAlR4oQocUKUOCHqbzvYTzLwTPegAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADd0lEQVR4nO3dQW1iUQCGUZh0DxKwgAMcFAs4wAFIQQI4oA4qoRJw8GqAtJPJ5PKVnrPkLf67+XITNnc+TdMM6Pnz6AMA94kTosQJUeKEKHFC1Ms33/2V+x9cLpdhW9vtdtjW4XAYtnU8HodtPcD83o9uTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0R99xzDU7per0P3Rj6RwPNwc0KUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCEq8xzDx8fHsC3PI/ATuDkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6I+pVvpez3+2Fbs9lsdr1eh229vb0N21qv18O2fiM3J0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6Lm0zR99f3Lj/yd0+k0bGu32w3ben19HbZ1Pp+HbT3A/N6Pbk6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEvTz6APxct9vt0Ud4am5OiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRHmOgX/2/v4+bGv00w/L5XLo3j1uTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6K8lTLAarUatrVYLIZtbTabYVveSgEyxAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR82maHn0G4A43J0SJE6LECVHihChxQpQ4IeoTkBs1JMeZZDcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADsklEQVR4nO3dsU1jQRRA0fGKHJdAB7gESnAJ7gBaoQM6ADqgFLdABX+jzSyzATv/8vec8E/wRpauRnLydsuyDKDn19oXAC4TJ0SJE6LECVHihKibL879lfsNTqfTtFnn83narIeHh2mznp6eps0aY4z9fj9z3O7SRy8nRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTonZfLM/d5DqG9/f3qfOOx+PUeVu08SXP1jHATyJOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiLpZ+wJreH19XfsKm/D4+Lj2FTbNywlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocULUblmWa+dXD7/T+XyeNWocDodps8YY4/Pzc9qs+/v7abPe3t6mzbq7u5s2awW7Sx+9nBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlToi6WfsCf8xcxzBzPcJsx+Nx2qyNr0hYnZcTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUZl1DIfDYdqs29vbabPGmLv+Yb/fT5vFv+XlhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQlRmHcPMNQKn02narDHGeH5+njbr4+Nj2qyZv+P/uGbCywlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocULUblmWa+dXD/k7Ly8vm5w1c1fK7P02k+0uffRyQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IeqrdQzASrycECVOiBInRIkTosQJUeKEqN8/4UQ1z9IVdgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "plot_digit(X_01[80])\n",
    "print(y_01[80])\n",
    "\n",
    "plot_digit(X_01[345])\n",
    "print(y_01[345])\n",
    "\n",
    "plot_digit(X_01[87])\n",
    "print(y_01[87])\n",
    "\n",
    "plot_digit(X_01[144])\n",
    "print(y_01[144])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of our dataset is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 64)\n",
      "(360,)\n"
     ]
    }
   ],
   "source": [
    "print(X_01.shape)\n",
    "print(y_01.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our reduced dataset how many 0 and 1 do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of 0: 178\n",
      "number of 1: 182\n"
     ]
    }
   ],
   "source": [
    "print(\"number of 0:\", np.count_nonzero(y_01 == 0))\n",
    "print(\"number of 1:\", np.count_nonzero(y_01 == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shuffle the elements and create a train and a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train is (251, 64)\n",
      "Shape of X_test is (109, 64)\n",
      "Shape of y_train is (251,)\n",
      "Shape of y_test is (109,)\n"
     ]
    }
   ],
   "source": [
    "shuffle_index = np.random.permutation(X_01.shape[0])\n",
    "X_01_shuffled, y_01_shuffled = X_01[shuffle_index], y_01[shuffle_index]\n",
    "\n",
    "train_proportion = 0.7\n",
    "train_test_cut = int(len(X_01)*train_proportion)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    X_01_shuffled[:train_test_cut], \\\n",
    "    X_01_shuffled[train_test_cut:], \\\n",
    "    y_01_shuffled[:train_test_cut], \\\n",
    "    y_01_shuffled[train_test_cut:]\n",
    "    \n",
    "print(\"Shape of X_train is\", X_train.shape)\n",
    "print(\"Shape of X_test is\", X_test.shape)\n",
    "print(\"Shape of y_train is\", y_train.shape)\n",
    "print(\"Shape of y_test is\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the proportions of 0 and 1 in our training and test set. In our original set we had the proportion of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.978021978021978"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(y_01 == 0) / np.count_nonzero(y_01 == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our training and test set we have the proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763779527559056\n",
      "0.9818181818181818\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(y_train == 0) / np.count_nonzero(y_train == 1))\n",
    "print(np.count_nonzero(y_test == 0) / np.count_nonzero(y_test == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are pretty close to the same proportion in our original dataset. Note that in this case is not so important since our training set is not so skewed, but normally you would have to do a stratified sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalise our data. The pixel will have a value between 0 and 255 (gray values). Let's normalise the value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalised = X_train/255.0\n",
    "X_test_normalised = X_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we need features along the rows, and training cases along the columns. So let's reshape our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 251)\n",
      "(1, 251)\n",
      "(64, 109)\n",
      "(1, 109)\n",
      "The training dataset has dimensions equal to 251\n",
      "The test set has dimensions equal to 109\n"
     ]
    }
   ],
   "source": [
    "X_train_tr = X_train_normalised.transpose()\n",
    "y_train_tr = y_train.reshape(1,y_train.shape[0])\n",
    "X_test_tr = X_test_normalised.transpose()\n",
    "y_test_tr = y_test.reshape(1,y_test.shape[0])\n",
    "\n",
    "print(X_train_tr.shape)\n",
    "print(y_train_tr.shape)\n",
    "print(X_test_tr.shape)\n",
    "print(y_test_tr.shape)\n",
    "\n",
    "dim_train = X_train_tr.shape[1]\n",
    "dim_test = X_test_tr.shape[1]\n",
    "\n",
    "print(\"The training dataset has dimensions equal to\", dim_train)\n",
    "print(\"The test set has dimensions equal to\", dim_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define a few helper function that we will put together to build a ```model()``` function that will train our model and give as a result a dictionary with the result. Where relevant we will put the mathematical formula we have used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function\n",
    "\n",
    "This function will calculate the following formula given an input $z$\n",
    "\n",
    "$$ \\displaystyle\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "\\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Implement the sigmoid function\n",
    "\n",
    "    Arguments:\n",
    "    y -- a scalar (float)\n",
    "\n",
    "    Return:\n",
    "    s -- the sigmoid function evaluated on z (as in equation (1))\n",
    "    \"\"\"\n",
    "    s = 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(dim):\n",
    "    \"\"\"\n",
    "    Initialise the weights and the bias to tensors of dimensions (dim,1) for w and\n",
    "    to 1 for b (a scalar)\n",
    "\n",
    "    Arguments:\n",
    "    dim -- a scalar (float)\n",
    "\n",
    "    Return:\n",
    "    w -- a matrix of dimensions (dim,1) containing all zero\n",
    "    b -- a scalar = 0\n",
    "    \"\"\"\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "    \n",
    "    assert (w.shape == (dim,1))\n",
    "    assert (isinstance(b, float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function ```propagate(w,b,X,Y)``` will calculate \n",
    "\n",
    "$$\n",
    "\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial w_j} = \\frac{1}{m} X (A-Y)^T\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m}(A_i-Y_i) \n",
    "\\tag{3}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\displaystyle\n",
    "J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathscr{L} (a^{(i)},y^{(i)})\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "given\n",
    "$$\n",
    "\\mathscr{L} (a,y) = -\\left[ y \\log a + (1-y) \\log (1-a) \\right]\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "Our inputs are the weight $w \\in \\mathbb{R}^{n_x \\times 1}$ ($n_x$ number of features), $b \\in \\mathbb{R}$, $X \\in \\mathbb{R}^{n_x \\times m}$, $Y \\in \\mathbb{R}^{1 \\times m}$, $m$ is the number of training cases we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px, 1) (our case 784,1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if class 0, 1 if class 1) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    z = np.dot(w.T,X)+b\n",
    "    A = sigmoid(z)\n",
    "    cost = -1.0/m*np.sum(Y*np.log(A)+(1.0-Y)*np.log(1.0-A))\n",
    "    \n",
    "    dw = 1.0/m*np.dot(X, (A-Y).T) #this is derivative of the cost function with respect to w\n",
    "    db = 1.0/m*np.sum(A-Y)       #this is the derivative of the cost function with respect to b\n",
    "    \n",
    "    assert (dw.shape == w.shape)\n",
    "    assert (db.dtype == float)\n",
    "    \n",
    "    cost = np.squeeze(cost)     \n",
    "    assert (cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw, #these values are stored in a dictionary so as to access them later\n",
    "             \"db\":db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function actually performs the gradient descent algorithm. It does a loop modifying the weights and the bias at each iteration according to (for an explanation of notation please refer to [this notebook](http://localhost:8888/notebooks/Documents/Data%20Science/Projects/Logistic-Regression-Explained/Logistic%20Regression%20from%20scratch.ipynb#).\n",
    "\n",
    "$$\\displaystyle\n",
    "w_{[n+1]} = w_{[n]}-\\alpha \\frac{1}{m} X (A-Y)^T\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\displaystyle\n",
    "b_{[n+1]} = b_{[n]}-\\alpha \\frac{1}{m} \\sum_{i=1}^{m}(A_i-Y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are trying to get the parameters w and b after modifying them using the knowledge of the cost function\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (n_x, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (n_x, m)\n",
    "    Y -- true \"label\" vector (containing 0 if class 1, 1 if class 1), of shape (1, m)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \"\"\"\n",
    "    costs = []     #This is an empty list created so that it stores all the values later\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate(w, b, X, Y)    #we are calling the previously defined function\n",
    "        \n",
    "        dw = grads[\"dw\"]   #we are accessing the derivatives of cost with respect to w\n",
    "        db = grads[\"db\"]   #we are accessing the derivatives of cost with respect to b\n",
    "        \n",
    "        w = w - learning_rate*dw   #we are modifying the parameter w so that the cost would reduce in the long run\n",
    "        b = b - learning_rate*db   #we are modifying the parameter b so that the cost would reduce in the long run\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)    #we are giving all the cost values to the empty list that was created initially\n",
    "            \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost (iteration %i) = %f\" %(i, cost))\n",
    "            \n",
    "    grads = {\"dw\": dw, \"db\": db}    #we are storing this value in the dictionary so that it could be accessed later\n",
    "    params = {\"w\": w, \"b\": b}    #we are storing this value in the dictionary so that it could be accessed later\n",
    "        \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ```predict()``` creates a matrix of dimensions $(1,m)$ that contains the predictions of the model given the input $w$, $b$ and $X$. Each prediction is assigned to class 0 if $\\sigma(w^T X+b) > 0.5$ and to class 1 if $\\sigma(w^T X+b) \\leq 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 \n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (n_x, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (n_x, m)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) \n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1] #alt m = len(X)\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A = sigmoid (np.dot(w.T, X)+b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        if (A[:,i] > 0.5): \n",
    "            Y_prediction[:, i] = 1\n",
    "        elif (A[:,i] <= 0.5):\n",
    "            Y_prediction[:, i] = 0\n",
    "            \n",
    "    assert (Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the ```model()``` function that will put everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model (X_train, Y_train, X_test, Y_test, num_iterations = 1000, learning_rate = 0.5, print_cost = False):\n",
    "    \n",
    "    w, b = initialize(X_train.shape[0])\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    Y_prediction_test = predict (w, b, X_test)\n",
    "    Y_prediction_train = predict (w, b, X_train)\n",
    "    \n",
    "    train_accuracy = 100.0 - np.mean(np.abs(Y_prediction_train-Y_train)*100.0)\n",
    "    test_accuracy = 100.0 - np.mean(np.abs(Y_prediction_test-Y_test)*100.0)\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "        \"Y_prediction_test\": Y_prediction_test,\n",
    "        \"Y_prediction_train\": Y_prediction_train,\n",
    "         \"w\": w,\n",
    "         \"b\": b,\n",
    "         \"learning_rate\": learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    print (\"Accuarcy Test: \",  test_accuracy)\n",
    "    print (\"Accuracy Train: \", train_accuracy)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of the model\n",
    "\n",
    "Let's test our model on our datasets containing only the digits 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost (iteration 0) = 0.693147\n",
      "Cost (iteration 100) = 0.684282\n",
      "Cost (iteration 200) = 0.675647\n",
      "Cost (iteration 300) = 0.667180\n",
      "Cost (iteration 400) = 0.658872\n",
      "Cost (iteration 500) = 0.650722\n",
      "Cost (iteration 600) = 0.642725\n",
      "Cost (iteration 700) = 0.634878\n",
      "Cost (iteration 800) = 0.627179\n",
      "Cost (iteration 900) = 0.619625\n",
      "Cost (iteration 1000) = 0.612212\n",
      "Cost (iteration 1100) = 0.604937\n",
      "Cost (iteration 1200) = 0.597799\n",
      "Cost (iteration 1300) = 0.590793\n",
      "Cost (iteration 1400) = 0.583918\n",
      "Cost (iteration 1500) = 0.577170\n",
      "Cost (iteration 1600) = 0.570547\n",
      "Cost (iteration 1700) = 0.564046\n",
      "Cost (iteration 1800) = 0.557665\n",
      "Cost (iteration 1900) = 0.551400\n",
      "Cost (iteration 2000) = 0.545251\n",
      "Cost (iteration 2100) = 0.539213\n",
      "Cost (iteration 2200) = 0.533286\n",
      "Cost (iteration 2300) = 0.527465\n",
      "Cost (iteration 2400) = 0.521750\n",
      "Cost (iteration 2500) = 0.516138\n",
      "Cost (iteration 2600) = 0.510626\n",
      "Cost (iteration 2700) = 0.505213\n",
      "Cost (iteration 2800) = 0.499896\n",
      "Cost (iteration 2900) = 0.494674\n",
      "Cost (iteration 3000) = 0.489544\n",
      "Cost (iteration 3100) = 0.484504\n",
      "Cost (iteration 3200) = 0.479552\n",
      "Cost (iteration 3300) = 0.474687\n",
      "Cost (iteration 3400) = 0.469906\n",
      "Cost (iteration 3500) = 0.465208\n",
      "Cost (iteration 3600) = 0.460591\n",
      "Cost (iteration 3700) = 0.456054\n",
      "Cost (iteration 3800) = 0.451594\n",
      "Cost (iteration 3900) = 0.447209\n",
      "Accuarcy Test:  99.08256880733946\n",
      "Accuracy Train:  99.60159362549801\n"
     ]
    }
   ],
   "source": [
    "d = model (X_train_tr, \n",
    "           y_train_tr, \n",
    "           X_test_tr, \n",
    "           y_test_tr, \n",
    "           num_iterations = 4000, \n",
    "           learning_rate = 0.05, \n",
    "           print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get an accuracy of roughly 99%. Pretty good. \n",
    "Let's see how the confusion matrix look like. To make it easier let's use the ```confusion_matrix()``` function from the package ```sklearn.metrics```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[54,  0],\n",
       "       [ 1, 54]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_tr = d[\"Y_prediction_test\"]\n",
    "y_pred_tr_ravel = y_pred_tr.ravel()\n",
    "y_test_tr_ravel = y_test_tr.ravel()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix (y_test_tr_ravel, y_pred_tr_ravel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this works really well since 0 and 1 are easy to distinguish. If you check all the digits you will see that (for example) two of the digits that are the most difficult (relatively) to distinguish are 3 and 5. The same notebook can be run with different digits to check this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function vs. number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the cost function decrease with the number of iterations. We have the value of the cost function every $100$ iterations so we can easily plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(d[\"costs\"])\n",
    "plt.xlim([1,40])\n",
    "plt.ylim([0,0.12])\n",
    "plt.title(\"Cost Function\",fontsize = 15)\n",
    "plt.xlabel(\"Number of iterations x 100\", fontsize = 14)\n",
    "plt.ylabel(\"$J(w,b)$\", fontsize = 17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
